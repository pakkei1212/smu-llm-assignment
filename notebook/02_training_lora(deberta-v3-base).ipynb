{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "486ee8cb-29b6-48cf-9691-6057d72a5a79",
   "metadata": {},
   "source": [
    "# LLM Fine-Tuning with Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35ea2dd-67fc-4214-827e-b550005344ef",
   "metadata": {},
   "source": [
    "## Environment & Version Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a118c5b-35df-4e52-a9ab-bfa5c2a7af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "import sys, os, json, copy\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments, DataCollatorWithPadding\n",
    "from src.EncoderTrainer import EncoderTrainer\n",
    "from peft import LoraConfig, TaskType\n",
    "import numpy as np\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6763ec-1f39-4f38-8266-b0283340119b",
   "metadata": {},
   "source": [
    "## Global Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6015a1e-cbd0-46db-bbe1-b2348370cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== paths & constants =====\n",
    "OUTPUT_FOLDER = \"../outputs\"\n",
    "DATA_FOLDER = \"../data\"\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "FOLDER_NAME = \"deberta-v3-base\"\n",
    "\n",
    "VAL_JSON  = f\"{DATA_FOLDER}/processed/val.json\"\n",
    "TEST_JSON = f\"{DATA_FOLDER}/processed/test.json\"\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "LABELS = [\"negative\", \"neutral\", \"positive\"]\n",
    "NUM_LABELS = len(LABELS)\n",
    "\n",
    "LORA_TUNING_DIR = f\"{OUTPUT_FOLDER}/lora_tuning/{FOLDER_NAME}\"\n",
    "os.makedirs(LORA_TUNING_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fffe8-ae6d-414d-ae83-0777d2b70121",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4e5f27-0916-480c-ad76-4b8bccbcd684",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": f\"{DATA_FOLDER}/processed/train.json\",\n",
    "        \"validation\": f\"{DATA_FOLDER}/processed/val.json\",\n",
    "        \"test\": f\"{DATA_FOLDER}/processed/test.json\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb5ac4e6-f0a0-4872-bc73-fa58159bfc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts: [ 34 143  65]\n",
      "Class Weights (Inverse Normalized): tensor([0.5679, 0.1350, 0.2971])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract outputs\n",
    "outputs = dataset['test']['output']\n",
    "\n",
    "# Count occurrences\n",
    "label_counts = Counter(outputs)\n",
    "total = sum(label_counts.values())\n",
    "\n",
    "counts = np.array([label_counts.get(label, 0) for label in LABELS])\n",
    "\n",
    "print(\"Counts:\", counts)\n",
    "\n",
    "inverse_weights = total / (len(LABELS) * np.maximum(counts, 1))\n",
    "\n",
    "# Normalize weights\n",
    "inverse_weights = inverse_weights / inverse_weights.sum()\n",
    "\n",
    "CLASS_WEIGHTS = torch.tensor(inverse_weights, dtype=torch.float)\n",
    "\n",
    "print(\"Class Weights (Inverse Normalized):\", CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250447c9-9697-480d-8077-6026360231cb",
   "metadata": {},
   "source": [
    "## Prompt Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01bce13-0f93-49e3-8774-40bae4043cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = EncoderTrainer(\n",
    "    model_name=MODEL_NAME,\n",
    "    labels=LABELS,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "\n",
    "tokenizer = trainer.tokenizer\n",
    "\n",
    "def format_encoder(example):\n",
    "    input = example[\"input\"]\n",
    "    label = LABELS.index(example[\"output\"])  # or map from output\n",
    "\n",
    "    return {\n",
    "        \"input\": input,\n",
    "        \"label\": label,\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_encoder,\n",
    "    batched=False,\n",
    "    num_proc=1,\n",
    "    desc=\"Formatting prompts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7711630-909f-426a-af20-5b6b8c2b7c02",
   "metadata": {},
   "source": [
    "## Tokenization & Data Collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b317f8d3-8ee2-4bcd-a60b-ac3f25669530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_encoder(batch):\n",
    "    enc = tokenizer(\n",
    "        batch[\"input\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    enc[\"labels\"] = int(batch[\"label\"])\n",
    "    return enc\n",
    "\n",
    "tokenized_ds = dataset.map(\n",
    "    tokenize_encoder,\n",
    "    batched=False,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing encoder inputs\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d9ade-76da-47ad-82ba-01746c2c7f40",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a840d5fe-8e03-47e6-9e34-50c0fc7e37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TRAINING_ARGS = dict(\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,   # effective batch = 8\n",
    "    num_train_epochs=4,\n",
    "    lr_scheduler_type=\"cosine\", \n",
    "    weight_decay=0.01,\n",
    "    # warmup_steps=100,\n",
    "    warmup_ratio=0.1,\n",
    "\n",
    "    # precision (BF16 ONLY)\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab33e9b-88d5-4519-8bcf-a4dc88608b37",
   "metadata": {},
   "source": [
    "## Define the Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959c5f1d-6371-4888-9d9b-ffb55c2e0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS = []\n",
    "\n",
    "# -------------------------\n",
    "# Base configuration\n",
    "# -------------------------\n",
    "best_cfg = {\n",
    "    \"use_lora\": True,              \n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"r\": 8,\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"use_class_weights\": False,\n",
    "}\n",
    "\n",
    "def run_experiment(cfg, stage_name):\n",
    "    tag = f\"{stage_name}_\" + \"_\".join([f\"{k}_{v}\" for k, v in cfg.items()])\n",
    "    out_dir = f\"{LORA_TUNING_DIR}/{tag}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"\\n===== {stage_name} | Running config: {cfg} =====\")\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Build TrainingArguments\n",
    "    # ----------------------------------\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        learning_rate=cfg[\"learning_rate\"],\n",
    "        **BASE_TRAINING_ARGS,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Initialize Trainer\n",
    "    # ----------------------------------\n",
    "    trainer = EncoderTrainer(\n",
    "        model_name=MODEL_NAME,\n",
    "        labels=LABELS,\n",
    "        load_in_4bit=False,\n",
    "    )\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Configure LoRA (if enabled)\n",
    "    # ----------------------------------\n",
    "    if cfg.get(\"use_lora\", True):\n",
    "        trainer.configure_lora(\n",
    "            r=cfg[\"r\"],\n",
    "            lora_alpha=cfg[\"lora_alpha\"],\n",
    "            lora_dropout=cfg[\"lora_dropout\"],\n",
    "            target_modules=[\"query_proj\", \"value_proj\"]\n",
    "        )\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Class weights\n",
    "    # ----------------------------------\n",
    "    if cfg[\"use_class_weights\"]:\n",
    "        trainer.class_weights = CLASS_WEIGHTS\n",
    "\n",
    "    # ----- Train -----\n",
    "    metrics = trainer.train(\n",
    "        train_dataset=tokenized_ds[\"train\"],\n",
    "        eval_dataset=tokenized_ds[\"validation\"],\n",
    "        training_args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        classification_eval_fn=lambda: trainer.evaluate_classification(\n",
    "            test_path=VAL_JSON,\n",
    "            labels=LABELS,\n",
    "            verbose=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Save model (FFT + LoRA unified)\n",
    "    # ----------------------------------\n",
    "    print(\"Before save:\",\n",
    "      trainer.model.classifier.weight.mean())\n",
    "\n",
    "    trainer.save_model(out_dir)\n",
    "    \n",
    "    # ----------------------------------\n",
    "    # Save metrics\n",
    "    # ----------------------------------\n",
    "    with open(os.path.join(out_dir, \"metrics.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Save experiment metadata (FULL INFO)\n",
    "    # ----------------------------------\n",
    "    experiment_metadata = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"experiment_type\": stage_name,\n",
    "        \"learning_rate\": cfg.get(\"learning_rate\"),\n",
    "        \"r\": cfg.get(\"r\"),\n",
    "        \"lora_alpha\": cfg.get(\"lora_alpha\"),\n",
    "        \"lora_dropout\": cfg.get(\"lora_dropout\"),\n",
    "        \"use_lora\": cfg.get(\"use_lora\"),\n",
    "        \"use_class_weights\": cfg.get(\"use_class_weights\"),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(out_dir, \"exp_config.json\"), \"w\") as f:\n",
    "        json.dump(experiment_metadata, f, indent=2)\n",
    "\n",
    "    # ----------------------------------\n",
    "    # Store in memory\n",
    "    # ----------------------------------\n",
    "    RESULTS.append({\n",
    "        \"stage\": stage_name,\n",
    "        \"config\": copy.deepcopy(cfg),\n",
    "        \"metrics\": metrics,\n",
    "        \"output_dir\": out_dir,\n",
    "    })\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaba394-be3c-4759-8079-d85cad2b3ac3",
   "metadata": {},
   "source": [
    "## Stage 1 — Full Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c5ce2b-c553-49ba-940e-900ef63c244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FFT_LR_5e-05 | Running config: {'use_lora': False, 'learning_rate': 5e-05, 'r': None, 'lora_alpha': None, 'lora_dropout': None, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 06:41, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>0.492738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.393013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274500</td>\n",
       "      <td>0.354403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.182900</td>\n",
       "      <td>0.355174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7851\n",
      "precision: 0.8313\n",
      "recall: 0.6184\n",
      "f1: 0.6269\n",
      "auc_ovr: 0.9300\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8760\n",
      "precision: 0.8455\n",
      "recall: 0.8944\n",
      "f1: 0.8665\n",
      "auc_ovr: 0.9700\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.9008\n",
      "precision: 0.8758\n",
      "recall: 0.9156\n",
      "f1: 0.8936\n",
      "auc_ovr: 0.9696\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.9008\n",
      "precision: 0.8758\n",
      "recall: 0.9156\n",
      "f1: 0.8936\n",
      "auc_ovr: 0.9694\n",
      "\n",
      "===== FFT_LR_0.0001 | Running config: {'use_lora': False, 'learning_rate': 0.0001, 'r': None, 'lora_alpha': None, 'lora_dropout': None, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 06:54, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.789400</td>\n",
       "      <td>0.623070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.488249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.295900</td>\n",
       "      <td>0.443855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.436957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7479\n",
      "precision: 0.4733\n",
      "recall: 0.5401\n",
      "f1: 0.5040\n",
      "auc_ovr: 0.8867\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8636\n",
      "precision: 0.8352\n",
      "recall: 0.8850\n",
      "f1: 0.8564\n",
      "auc_ovr: 0.9579\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8843\n",
      "precision: 0.8600\n",
      "recall: 0.8919\n",
      "f1: 0.8746\n",
      "auc_ovr: 0.9580\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8884\n",
      "precision: 0.8689\n",
      "recall: 0.8943\n",
      "f1: 0.8808\n",
      "auc_ovr: 0.9580\n",
      "\n",
      "===== FFT_LR_0.0002 | Running config: {'use_lora': False, 'learning_rate': 0.0002, 'r': None, 'lora_alpha': None, 'lora_dropout': None, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 06:57, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.951700</td>\n",
       "      <td>0.855348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.784700</td>\n",
       "      <td>0.920697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.871300</td>\n",
       "      <td>0.873712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.857400</td>\n",
       "      <td>0.869512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5785\n",
      "precision: 0.3878\n",
      "recall: 0.4398\n",
      "f1: 0.3990\n",
      "auc_ovr: 0.6516\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5928\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6240\n",
      "precision: 0.3771\n",
      "recall: 0.4158\n",
      "f1: 0.3918\n",
      "auc_ovr: 0.6954\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6281\n",
      "precision: 0.3802\n",
      "recall: 0.4181\n",
      "f1: 0.3941\n",
      "auc_ovr: 0.6801\n",
      "\n",
      "===== FFT_LR_0.0003 | Running config: {'use_lora': False, 'learning_rate': 0.0003, 'r': None, 'lora_alpha': None, 'lora_dropout': None, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 06:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.966400</td>\n",
       "      <td>0.943480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.914100</td>\n",
       "      <td>0.933680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.938100</td>\n",
       "      <td>0.919472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.967500</td>\n",
       "      <td>0.917990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5082\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5811\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.6153\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.6128\n",
      "\n",
      "===== FFT_LR_0.0005 | Running config: {'use_lora': False, 'learning_rate': 0.0005, 'r': None, 'lora_alpha': None, 'lora_dropout': None, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 06:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.969600</td>\n",
       "      <td>0.948234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>0.934408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.937100</td>\n",
       "      <td>0.914556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.967300</td>\n",
       "      <td>0.920475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5339\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.6278\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.6108\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5364\n",
      "\n",
      "===== FFT_LR_0.0006 | Running config: {'use_lora': False, 'learning_rate': 0.0006, 'r': None, 'lora_alpha': None, 'lora_dropout': None, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 06:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.969300</td>\n",
       "      <td>0.951288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.915500</td>\n",
       "      <td>0.938651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.938100</td>\n",
       "      <td>0.922399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.968300</td>\n",
       "      <td>0.921514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.4904\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5111\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5054\n",
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.5023\n",
      "Best FFT LR: 5e-05\n"
     ]
    }
   ],
   "source": [
    "full_ft_cfg = {\n",
    "    \"use_lora\": False,      # Full FT\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"r\": None,\n",
    "    \"lora_alpha\": None,\n",
    "    \"lora_dropout\": None,\n",
    "    \"use_class_weights\": False,\n",
    "}\n",
    "\n",
    "lr_candidates = [5e-5, 1e-4, 2e-4, 3e-4, 5e-4, 6e-4]\n",
    "best_metric = -1\n",
    "best_lr = None\n",
    "\n",
    "for lr in lr_candidates:\n",
    "    cfg = copy.deepcopy(full_ft_cfg)\n",
    "    cfg[\"learning_rate\"] = lr\n",
    "\n",
    "    metrics = run_experiment(cfg, f\"FFT_LR_{lr}\")\n",
    "    score = metrics[\"f1\"]   # use macro F1 ideally\n",
    "\n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        best_lr = lr\n",
    "\n",
    "print(\"Best FFT LR:\", best_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a634b1-5fd9-4bf6-8f2a-32253fc6537f",
   "metadata": {},
   "source": [
    "## Stage 2 — LoRA (Tune Learning Rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cee231a-ed69-4e2d-8a04-ca47059de632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_LR_5e-05 | Running config: {'use_lora': True, 'learning_rate': 5e-05, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.003800</td>\n",
       "      <td>0.930020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.826200</td>\n",
       "      <td>0.843596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.859100</td>\n",
       "      <td>0.831479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>0.830839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.7104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.7514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.7624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.7635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_LR_0.0001 | Running config: {'use_lora': True, 'learning_rate': 0.0001, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:43, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.924700</td>\n",
       "      <td>0.852881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.707500</td>\n",
       "      <td>0.746384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.737000</td>\n",
       "      <td>0.701291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.708400</td>\n",
       "      <td>0.699855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.5909\n",
      "precision: 0.1970\n",
      "recall: 0.3333\n",
      "f1: 0.2476\n",
      "auc_ovr: 0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6901\n",
      "precision: 0.4258\n",
      "recall: 0.4578\n",
      "f1: 0.4330\n",
      "auc_ovr: 0.8123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6901\n",
      "precision: 0.4206\n",
      "recall: 0.4602\n",
      "f1: 0.4343\n",
      "auc_ovr: 0.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6901\n",
      "precision: 0.4206\n",
      "recall: 0.4602\n",
      "f1: 0.4343\n",
      "auc_ovr: 0.8390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_LR_0.0002 | Running config: {'use_lora': True, 'learning_rate': 0.0002, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:50, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.758514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.539900</td>\n",
       "      <td>0.559263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.612100</td>\n",
       "      <td>0.520731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.518468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6818\n",
      "precision: 0.4310\n",
      "recall: 0.4461\n",
      "f1: 0.4225\n",
      "auc_ovr: 0.7923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7686\n",
      "precision: 0.8217\n",
      "recall: 0.5899\n",
      "f1: 0.5823\n",
      "auc_ovr: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7686\n",
      "precision: 0.8209\n",
      "recall: 0.5661\n",
      "f1: 0.5413\n",
      "auc_ovr: 0.9027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7686\n",
      "precision: 0.8209\n",
      "recall: 0.5661\n",
      "f1: 0.5413\n",
      "auc_ovr: 0.9031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_LR_0.0003 | Running config: {'use_lora': True, 'learning_rate': 0.0003, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.755900</td>\n",
       "      <td>0.624615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.521400</td>\n",
       "      <td>0.507792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.401503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.400810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7066\n",
      "precision: 0.4302\n",
      "recall: 0.4719\n",
      "f1: 0.4453\n",
      "auc_ovr: 0.8654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7934\n",
      "precision: 0.7247\n",
      "recall: 0.7310\n",
      "f1: 0.7278\n",
      "auc_ovr: 0.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8512\n",
      "precision: 0.8761\n",
      "recall: 0.8017\n",
      "f1: 0.8308\n",
      "auc_ovr: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8512\n",
      "precision: 0.8759\n",
      "recall: 0.8112\n",
      "f1: 0.8374\n",
      "auc_ovr: 0.9486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_LR_0.0005 | Running config: {'use_lora': True, 'learning_rate': 0.0005, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.745500</td>\n",
       "      <td>0.624204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.405358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.391400</td>\n",
       "      <td>0.361064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.362144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7190\n",
      "precision: 0.4469\n",
      "recall: 0.5025\n",
      "f1: 0.4729\n",
      "auc_ovr: 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8760\n",
      "precision: 0.8754\n",
      "recall: 0.8419\n",
      "f1: 0.8569\n",
      "auc_ovr: 0.9498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8884\n",
      "precision: 0.8758\n",
      "recall: 0.8560\n",
      "f1: 0.8649\n",
      "auc_ovr: 0.9553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8802\n",
      "precision: 0.8690\n",
      "recall: 0.8490\n",
      "f1: 0.8580\n",
      "auc_ovr: 0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_LR_0.0006 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.646620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.367829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.341840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.350998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7149\n",
      "precision: 0.4471\n",
      "recall: 0.4978\n",
      "f1: 0.4702\n",
      "auc_ovr: 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8843\n",
      "precision: 0.8661\n",
      "recall: 0.8632\n",
      "f1: 0.8644\n",
      "auc_ovr: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.9008\n",
      "precision: 0.8969\n",
      "recall: 0.8821\n",
      "f1: 0.8892\n",
      "auc_ovr: 0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8967\n",
      "precision: 0.8913\n",
      "recall: 0.8894\n",
      "f1: 0.8903\n",
      "auc_ovr: 0.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LR: 0.0006\n"
     ]
    }
   ],
   "source": [
    "lr_candidates = [5e-5, 1e-4, 2e-4, 3e-4, 5e-4, 6e-4]\n",
    "best_metric = -1\n",
    "\n",
    "for lr in lr_candidates:\n",
    "    cfg = copy.deepcopy(best_cfg)\n",
    "    cfg[\"learning_rate\"] = lr\n",
    "\n",
    "    metrics = run_experiment(cfg, f\"LORA_LR_{lr}\")\n",
    "    score = metrics[\"f1\"]\n",
    "    \n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        best_cfg[\"learning_rate\"] = lr\n",
    "\n",
    "print(\"Best LR:\", best_cfg[\"learning_rate\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c86c73-ac0e-4d02-a1ed-e2f21858f704",
   "metadata": {},
   "source": [
    "## Stage 3 — LoRA (Tune Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92561173-332b-41c7-8c0b-5dcc53706c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_RANK_4 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 4, 'lora_alpha': 8, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 149,763 || all params: 184,574,214 || trainable%: 0.08113971976605573\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 05:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.607552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.480700</td>\n",
       "      <td>0.473501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.447300</td>\n",
       "      <td>0.426980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.422223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7190\n",
      "precision: 0.4485\n",
      "recall: 0.5049\n",
      "f1: 0.4749\n",
      "auc_ovr: 0.8609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8264\n",
      "precision: 0.7866\n",
      "recall: 0.7902\n",
      "f1: 0.7883\n",
      "auc_ovr: 0.9209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8678\n",
      "precision: 0.8881\n",
      "recall: 0.8492\n",
      "f1: 0.8651\n",
      "auc_ovr: 0.9434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8760\n",
      "precision: 0.8944\n",
      "recall: 0.8609\n",
      "f1: 0.8732\n",
      "auc_ovr: 0.9461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_RANK_8 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.646620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.367829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.341840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.350998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7149\n",
      "precision: 0.4471\n",
      "recall: 0.4978\n",
      "f1: 0.4702\n",
      "auc_ovr: 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8843\n",
      "precision: 0.8661\n",
      "recall: 0.8632\n",
      "f1: 0.8644\n",
      "auc_ovr: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.9008\n",
      "precision: 0.8969\n",
      "recall: 0.8821\n",
      "f1: 0.8892\n",
      "auc_ovr: 0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8967\n",
      "precision: 0.8913\n",
      "recall: 0.8894\n",
      "f1: 0.8903\n",
      "auc_ovr: 0.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_RANK_16 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 16, 'lora_alpha': 32, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 592,131 || all params: 185,016,582 || trainable%: 0.320042124656697\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:50, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.750300</td>\n",
       "      <td>0.654235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.362700</td>\n",
       "      <td>0.377403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329500</td>\n",
       "      <td>0.384532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.411137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7231\n",
      "precision: 0.4615\n",
      "recall: 0.5308\n",
      "f1: 0.4911\n",
      "auc_ovr: 0.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8595\n",
      "precision: 0.8494\n",
      "recall: 0.8494\n",
      "f1: 0.8494\n",
      "auc_ovr: 0.9501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8884\n",
      "precision: 0.8657\n",
      "recall: 0.8919\n",
      "f1: 0.8774\n",
      "auc_ovr: 0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8802\n",
      "precision: 0.8571\n",
      "recall: 0.8873\n",
      "f1: 0.8708\n",
      "auc_ovr: 0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Rank: 8\n"
     ]
    }
   ],
   "source": [
    "rank_candidates = [4, 8, 16]\n",
    "best_metric = -1\n",
    "\n",
    "for r in rank_candidates:\n",
    "    cfg = copy.deepcopy(best_cfg)\n",
    "    cfg[\"r\"] = r\n",
    "    cfg[\"lora_alpha\"] = 2 * r\n",
    "\n",
    "    metrics = run_experiment(cfg, f\"LORA_RANK_{r}\")\n",
    "    score = metrics[\"f1\"]\n",
    "    \n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        best_cfg[\"r\"] = r\n",
    "        best_cfg[\"lora_alpha\"] = 2 * r\n",
    "\n",
    "print(\"Best Rank:\", best_cfg[\"r\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18de651f-0e4d-4c75-8cc0-bfbe2f8d2d05",
   "metadata": {},
   "source": [
    "## Stage 4 — LoRA (Tune Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "917c5979-8b2d-42b8-83c7-37352dff6e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_ALPHA_8 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 8, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.758100</td>\n",
       "      <td>0.635214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.476800</td>\n",
       "      <td>0.424465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.406100</td>\n",
       "      <td>0.378098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.230700</td>\n",
       "      <td>0.373700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7479\n",
      "precision: 0.4752\n",
      "recall: 0.5236\n",
      "f1: 0.4965\n",
      "auc_ovr: 0.8534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8595\n",
      "precision: 0.8472\n",
      "recall: 0.8183\n",
      "f1: 0.8310\n",
      "auc_ovr: 0.9341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8802\n",
      "precision: 0.8994\n",
      "recall: 0.8417\n",
      "f1: 0.8640\n",
      "auc_ovr: 0.9552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8760\n",
      "precision: 0.8827\n",
      "recall: 0.8418\n",
      "f1: 0.8569\n",
      "auc_ovr: 0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_ALPHA_16 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 05:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.646620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.367829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.341840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.350998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7149\n",
      "precision: 0.4471\n",
      "recall: 0.4978\n",
      "f1: 0.4702\n",
      "auc_ovr: 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8843\n",
      "precision: 0.8661\n",
      "recall: 0.8632\n",
      "f1: 0.8644\n",
      "auc_ovr: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.9008\n",
      "precision: 0.8969\n",
      "recall: 0.8821\n",
      "f1: 0.8892\n",
      "auc_ovr: 0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8967\n",
      "precision: 0.8913\n",
      "recall: 0.8894\n",
      "f1: 0.8903\n",
      "auc_ovr: 0.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_ALPHA_32 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:55, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.840100</td>\n",
       "      <td>0.729153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.512800</td>\n",
       "      <td>0.476430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.407300</td>\n",
       "      <td>0.338555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.364118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7107\n",
      "precision: 0.4490\n",
      "recall: 0.4955\n",
      "f1: 0.4694\n",
      "auc_ovr: 0.8044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8223\n",
      "precision: 0.8584\n",
      "recall: 0.7374\n",
      "f1: 0.7681\n",
      "auc_ovr: 0.9223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8926\n",
      "precision: 0.8968\n",
      "recall: 0.8487\n",
      "f1: 0.8699\n",
      "auc_ovr: 0.9609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8926\n",
      "precision: 0.8944\n",
      "recall: 0.8679\n",
      "f1: 0.8802\n",
      "auc_ovr: 0.9620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha: 16\n"
     ]
    }
   ],
   "source": [
    "alpha_candidates = [\n",
    "    best_cfg[\"r\"],\n",
    "    2 * best_cfg[\"r\"],\n",
    "    4 * best_cfg[\"r\"]\n",
    "]\n",
    "\n",
    "best_metric = -1\n",
    "\n",
    "for alpha in alpha_candidates:\n",
    "    cfg = copy.deepcopy(best_cfg)\n",
    "    cfg[\"lora_alpha\"] = alpha\n",
    "\n",
    "    metrics = run_experiment(cfg, f\"LORA_ALPHA_{alpha}\")\n",
    "    score = metrics[\"f1\"]\n",
    "    \n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        best_cfg[\"lora_alpha\"] = alpha\n",
    "\n",
    "print(\"Best Alpha:\", best_cfg[\"lora_alpha\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e534de7-12a3-40aa-ae36-55bc9049483f",
   "metadata": {},
   "source": [
    "## Stage 5 — LoRA (Tune Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26747578-1965-4435-8115-29dfa8a4921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_DROPOUT_0.0 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.0, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:52, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.778600</td>\n",
       "      <td>0.669886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.486300</td>\n",
       "      <td>0.444559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.401600</td>\n",
       "      <td>0.388610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245100</td>\n",
       "      <td>0.383590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7273\n",
      "precision: 0.4536\n",
      "recall: 0.5024\n",
      "f1: 0.4752\n",
      "auc_ovr: 0.8501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8430\n",
      "precision: 0.8562\n",
      "recall: 0.8088\n",
      "f1: 0.8242\n",
      "auc_ovr: 0.9334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8719\n",
      "precision: 0.8607\n",
      "recall: 0.8490\n",
      "f1: 0.8528\n",
      "auc_ovr: 0.9576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8760\n",
      "precision: 0.8579\n",
      "recall: 0.8609\n",
      "f1: 0.8586\n",
      "auc_ovr: 0.9599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_DROPOUT_0.05 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:44, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742500</td>\n",
       "      <td>0.646620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.364600</td>\n",
       "      <td>0.367829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>0.341840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.350998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7149\n",
      "precision: 0.4471\n",
      "recall: 0.4978\n",
      "f1: 0.4702\n",
      "auc_ovr: 0.8436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8843\n",
      "precision: 0.8661\n",
      "recall: 0.8632\n",
      "f1: 0.8644\n",
      "auc_ovr: 0.9574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.9008\n",
      "precision: 0.8969\n",
      "recall: 0.8821\n",
      "f1: 0.8892\n",
      "auc_ovr: 0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8967\n",
      "precision: 0.8913\n",
      "recall: 0.8894\n",
      "f1: 0.8903\n",
      "auc_ovr: 0.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_DROPOUT_0.1 | Running config: {'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.1, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.830400</td>\n",
       "      <td>0.788386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.546837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.553100</td>\n",
       "      <td>0.463080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.347100</td>\n",
       "      <td>0.439963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6901\n",
      "precision: 0.4352\n",
      "recall: 0.4602\n",
      "f1: 0.4374\n",
      "auc_ovr: 0.6850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7810\n",
      "precision: 0.5049\n",
      "recall: 0.5871\n",
      "f1: 0.5384\n",
      "auc_ovr: 0.8831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8554\n",
      "precision: 0.8504\n",
      "recall: 0.7847\n",
      "f1: 0.8077\n",
      "auc_ovr: 0.9222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8636\n",
      "precision: 0.8872\n",
      "recall: 0.8133\n",
      "f1: 0.8388\n",
      "auc_ovr: 0.9314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Dropout: 0.05\n"
     ]
    }
   ],
   "source": [
    "dropout_candidates = [0.0, 0.05, 0.1]\n",
    "best_metric = -1\n",
    "\n",
    "for d in dropout_candidates:\n",
    "    cfg = copy.deepcopy(best_cfg)\n",
    "    cfg[\"lora_dropout\"] = d\n",
    "\n",
    "    metrics = run_experiment(cfg, f\"LORA_DROPOUT_{d}\")\n",
    "    score = metrics[\"f1\"]\n",
    "    \n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        best_cfg[\"lora_dropout\"] = d\n",
    "\n",
    "print(\"Best Dropout:\", best_cfg[\"lora_dropout\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42e801-7e11-43ef-a689-003de04357ed",
   "metadata": {},
   "source": [
    "## Stage 6 — LoRA (Class Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa09e3d8-0e10-44fd-9733-bc2b748c6b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_CLASS_WEIGHT | Running config: {'use_lora': True, 'learning_rate': 0.0002, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.751567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.549800</td>\n",
       "      <td>0.545538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.602700</td>\n",
       "      <td>0.518142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>0.518353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6777\n",
      "precision: 0.4228\n",
      "recall: 0.4485\n",
      "f1: 0.4249\n",
      "auc_ovr: 0.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7603\n",
      "precision: 0.8159\n",
      "recall: 0.5733\n",
      "f1: 0.5567\n",
      "auc_ovr: 0.8813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7727\n",
      "precision: 0.4936\n",
      "recall: 0.5683\n",
      "f1: 0.5264\n",
      "auc_ovr: 0.8950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7686\n",
      "precision: 0.4902\n",
      "recall: 0.5636\n",
      "f1: 0.5227\n",
      "auc_ovr: 0.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.utils.other.ModulesToSaveWrapper'>\n",
      "Before save: tensor(0.0005, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== LORA_CLASS_WEIGHT | Running config: {'use_lora': True, 'learning_rate': 0.0002, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': True} =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 297,219 || all params: 184,721,670 || trainable%: 0.1609009922874777\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 04:33, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.020100</td>\n",
       "      <td>0.901173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.692900</td>\n",
       "      <td>0.598332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.586000</td>\n",
       "      <td>0.426829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.421008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.6653\n",
      "precision: 0.4097\n",
      "recall: 0.4462\n",
      "f1: 0.4221\n",
      "auc_ovr: 0.7590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.7603\n",
      "precision: 0.7191\n",
      "recall: 0.7675\n",
      "f1: 0.7390\n",
      "auc_ovr: 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8058\n",
      "precision: 0.8025\n",
      "recall: 0.8167\n",
      "f1: 0.8069\n",
      "auc_ovr: 0.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Classification Metrics]\n",
      "accuracy: 0.8099\n",
      "precision: 0.8144\n",
      "recall: 0.8214\n",
      "f1: 0.8148\n",
      "auc_ovr: 0.9453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'peft.utils.other.ModulesToSaveWrapper'>\n",
      "Before save: tensor(-4.7684e-05, device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best use_class_weights: True\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "\n",
    "for use_weights in [False, True]:\n",
    "    cfg = copy.deepcopy(best_cfg)\n",
    "    cfg[\"use_class_weights\"] = use_weights\n",
    "\n",
    "    metrics = run_experiment(cfg, f\"LORA_CLASS_WEIGHT\")\n",
    "    score = metrics[\"f1\"]\n",
    "\n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        best_cfg[\"use_class_weights\"] = use_weights\n",
    "\n",
    "print(\"Best use_class_weights:\", best_cfg[\"use_class_weights\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "644bf226-fc44-4559-8613-b4126727bd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== FINAL BEST CONFIG =====\n",
      "{'use_lora': True, 'learning_rate': 0.0006, 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'use_class_weights': False}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== FINAL BEST CONFIG =====\")\n",
    "print(best_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19379de8-b935-479e-a786-46baca8c24d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
